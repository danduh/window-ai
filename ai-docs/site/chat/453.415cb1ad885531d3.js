"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[453],{9453:(e,n,t)=>{t.r(n),t.d(n,{default:()=>o});const o='# Prompt API\n\n## Prerequisites\n\n### Enable Gemini Nano and the Prompt API\n\nFollow these steps to enable Gemini Nano and the Prompt API flags for local experimentation:\n1. Open a new tab in Chrome, go to chrome://flags/#optimization-guide-on-device-model\n2. Select Enabled BypassPerfRequirement \n   - This bypass performance checks which might get in the way of having Gemini Nano downloaded on your device.\n3. Go to chrome://flags/#prompt-api-for-gemini-nano\n4. Select Enabled\n5. Relaunch Chrome.\n\n### Confirm availability of Gemini Nano\n1. Open DevTools and send `(await ai.languageModel.capabilities()).available;` in the console.\n2. If this returns \u201c_readily_\u201d, then you are all set.\n\n\n### Zero-shot prompting\n\nIn this example, a single string is used to prompt the API, which is assumed to come from the user. The returned response is from the language model.\n\n```js\nconst session = await ai.languageModel.create();\n\n// Prompt the model and wait for the whole result to come back.\nconst result = await session.prompt("Write me a poem.");\nconsole.log(result);\n\n// Prompt the model and stream the result:\nconst stream = await session.promptStreaming("Write me an extra-long poem.");\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n### System prompts\n\nThe language model can be configured with a special "system prompt" which gives it the context for future interactions:\n\n```js\nconst session = await ai.languageModel.create({\n  systemPrompt: "Pretend to be an eloquent hamster."\n});\n\nconsole.log(await session.prompt("What is your favorite food?"));\n```\n\nThe system prompt is special, in that the language model will not respond to it, and it will be preserved even if the context window otherwise overflows due to too many calls to `prompt()`.\n\nIf the system prompt is too large (see [below](#tokenization-context-window-length-limits-and-overflow)), then the promise will be rejected with a `"QuotaExceededError"` `DOMException`.\n\n### N-shot prompting\n\nIf developers want to provide examples of the user/assistant interaction, they can use the `initialPrompts` array. This aligns with the common "chat completions API" format of `{ role, content }` pairs, including a `"system"` role which can be used instead of the `systemPrompt` option shown above.\n\n```js\nconst session = await ai.languageModel.create({\n  initialPrompts: [\n    { role: "system", content: "Predict up to 5 emojis as a response to a comment. Output emojis, comma-separated." },\n    { role: "user", content: "This is amazing!" },\n    { role: "assistant", content: "\u2764\ufe0f, \u2795" },\n    { role: "user", content: "LGTM" },\n    { role: "assistant", content: "\ud83d\udc4d, \ud83d\udea2" }\n  ]\n});\n\n// Clone an existing session for efficiency, instead of recreating one each time.\nasync function predictEmoji(comment) {\n  const freshSession = await session.clone();\n  return await freshSession.prompt(comment);\n}\n\nconst result1 = await predictEmoji("Back to the drawing board");\n\nconst result2 = await predictEmoji("This code is so good you should get promoted");\n```\n\n(Note that merely creating a session does not cause any new responses from the language model. We need to call `prompt()` or `promptStreaming()` to get a response.)\n\nSome details on error cases:\n\n* Using both `systemPrompt` and a `{ role: "system" }` prompt in `initialPrompts`, or using multiple `{ role: "system" }` prompts, or placing the `{ role: "system" }` prompt anywhere besides at the 0th position in `initialPrompts`, will reject with a `TypeError`.\n* If the combined token length of all the initial prompts (including the separate `systemPrompt`, if provided) is too large, then the promise will be rejected with a `"QuotaExceededError"` `DOMException`.\n\n### Customizing the role per prompt\n\nOur examples so far have provided `prompt()` and `promptStreaming()` with a single string. Such cases assume messages will come from the user role. These methods can also take in objects in the `{ role, content }` format, or arrays of such objects, in case you want to provide multiple user or assistant messages before getting another assistant message:\n\n```js\nconst multiUserSession = await ai.languageModel.create({\n  systemPrompt: "You are a mediator in a discussion between two departments."\n});\n\nconst result = await multiUserSession.prompt([\n  { role: "user", content: "Marketing: We need more budget for advertising campaigns." },\n  { role: "user", content: "Finance: We need to cut costs and advertising is on the list." },\n  { role: "assistant", content: "Let\'s explore a compromise that satisfies both departments." }\n]);\n\n// `result` will contain a compromise proposal from the assistant.\n```\n\nBecause of their special behavior of being preserved on context window overflow, system prompts cannot be provided this way.\n\n### Emulating tool use or function-calling via assistant-role prompts\n\nA special case of the above is using the assistant role to emulate tool use or function-calling, by marking a response as coming from the assistant side of the conversation:\n\n```js\nconst session = await ai.languageModel.create({\n  systemPrompt: `\n    You are a helpful assistant. You have access to the following tools:\n    - calculator: A calculator. To use it, write "CALCULATOR: <expression>" where <expression> is a valid mathematical expression.\n  `\n});\n\nasync function promptWithCalculator(prompt) {\n  const result = await session.prompt(prompt);\n\n  // Check if the assistant wants to use the calculator tool.\n  const match = /^CALCULATOR: (.*)$/.exec(result);\n  if (match) {\n    const expression = match[1];\n    const mathResult = evaluateMathExpression(expression);\n\n    // Add the result to the session so it\'s in context going forward.\n    await session.prompt({ role: "assistant", content: mathResult });\n\n    // Return it as if that\'s what the assistant said to the user.\n    return mathResult;\n  }\n\n  // The assistant didn\'t want to use the calculator. Just return its response.\n  return result;\n}\n\nconsole.log(await promptWithCalculator("What is 2 + 2?"));\n```\n\nWe\'ll likely explore more specific APIs for tool- and function-calling in the future; follow along in [issue #7](https://github.com/explainers-by-googlers/prompt-api/issues/7).\n\n### Configuration of per-session options\n\nIn addition to the `systemPrompt` and `initialPrompts` options shown above, the currently-configurable options are [temperature](https://huggingface.co/blog/how-to-generate#sampling) and [top-K](https://huggingface.co/blog/how-to-generate#top-k-sampling). More information about the values for these parameters can be found using the `capabilities()` API explained [below](#capabilities-detection).\n\n```js\nconst customSession = await ai.languageModel.create({\n  temperature: 0.8,\n  topK: 10\n});\n\nconst capabilities = await ai.languageModel.capabilities();\nconst slightlyHighTemperatureSession = await ai.languageModel.create({\n  temperature: Math.max(\n    capabilities.defaultTemperature * 1.2,\n    capabilities.maxTemperature\n  ),\n  topK: 10\n});\n\n// capabilities also contains defaultTopK and maxTopK.\n```\n\n### Session persistence and cloning\n\nEach language model session consists of a persistent series of interactions with the model:\n\n```js\nconst session = await ai.languageModel.create({\n  systemPrompt: "You are a friendly, helpful assistant specialized in clothing choices."\n});\n\nconst result = await session.prompt(`\n  What should I wear today? It\'s sunny and I\'m unsure between a t-shirt and a polo.\n`);\n\nconsole.log(result);\n\nconst result2 = await session.prompt(`\n  That sounds great, but oh no, it\'s actually going to rain! New advice??\n`);\n```\n\nMultiple unrelated continuations of the same prompt can be set up by creating a session and then cloning it:\n\n```js\nconst session = await ai.languageModel.create({\n  systemPrompt: "You are a friendly, helpful assistant specialized in clothing choices."\n});\n\nconst session2 = await session.clone();\n```\n\nThe clone operation can be aborted using an `AbortSignal`:\n\n```js\nconst controller = new AbortController();\nconst session2 = await session.clone({ signal: controller.signal });\n```\n\n### Session destruction\n\nA language model session can be destroyed, either by using an `AbortSignal` passed to the `create()` method call:\n\n```js\nconst controller = new AbortController();\nstopButton.onclick = () => controller.abort();\n\nconst session = await ai.languageModel.create({ signal: controller.signal });\n```\n\nor by calling `destroy()` on the session:\n\n```js\nstopButton.onclick = () => session.destroy();\n```\n\nDestroying a session will have the following effects:\n\n* If done before the promise returned by `create()` is settled:\n\n    * Stop signaling any ongoing download progress for the language model. (The browser may also abort the download, or may continue it. Either way, no further `downloadprogress` events will fire.)\n\n    * Reject the `create()` promise.\n\n* Otherwise:\n\n    * Reject any ongoing calls to `prompt()`.\n\n    * Error any `ReadableStream`s returned by `promptStreaming()`.\n\n* Most importantly, destroying the session allows the user agent to unload the language model from memory, if no other APIs or sessions are using it.\n\nIn all cases the exception used for rejecting promises or erroring `ReadableStream`s will be an `"AbortError"` `DOMException`, or the given abort reason.\n\nThe ability to manually destroy a session allows applications to free up memory without waiting for garbage collection, which can be useful since language models can be quite large.\n\n### Aborting a specific prompt\n\nSpecific calls to `prompt()` or `promptStreaming()` can be aborted by passing an `AbortSignal` to them:\n\n```js\nconst controller = new AbortController();\nstopButton.onclick = () => controller.abort();\n\nconst result = await session.prompt("Write me a poem", { signal: controller.signal });\n```\n\nNote that because sessions are stateful, and prompts can be queued, aborting a specific prompt is slightly complicated:\n\n* If the prompt is still queued behind other prompts in the session, then it will be removed from the queue.\n* If the prompt is being currently processed by the model, then it will be aborted, and the prompt/response pair will be removed from the conversation history.\n* If the prompt has already been fully processed by the model, then attempting to abort the prompt will do nothing.\n\n### Tokenization, context window length limits, and overflow\n\nA given language model session will have a maximum number of tokens it can process. Developers can check their current usage and progress toward that limit by using the following properties on the session object:\n\n```js\nconsole.log(`${session.tokensSoFar}/${session.maxTokens} (${session.tokensLeft} left)`);\n```\n\nTo know how many tokens a string will consume, without actually processing it, developers can use the `countPromptTokens()` method:\n\n```js\nconst numTokens = await session.countPromptTokens(promptString);\n```\n\nSome notes on this API:\n\n* We do not expose the actual tokenization to developers since that would make it too easy to depend on model-specific details.\n* Implementations must include in their count any control tokens that will be necessary to process the prompt, e.g. ones indicating the start or end of the input.\n* The counting process can be aborted by passing an `AbortSignal`, i.e. `session.countPromptTokens(promptString, { signal })`.\n\nIt\'s possible to send a prompt that causes the context window to overflow. That is, consider a case where `session.countPromptTokens(promptString) > session.tokensLeft` before calling `session.prompt(promptString)`, and then the web developer calls `session.prompt(promptString)` anyway. In such cases, the initial portions of the conversation with the language model will be removed, one prompt/response pair at a time, until enough tokens are available to process the new prompt. The exception is the [system prompt](#system-prompts), which is never removed. If it\'s not possible to remove enough tokens from the conversation history to process the new prompt, then the `prompt()` or `promptStreaming()` call will fail with an `"QuotaExceededError"` `DOMException` and nothing will be removed.\n\nSuch overflows can be detected by listening for the `"contextoverflow"` event on the session:\n\n```js\nsession.addEventListener("contextoverflow", () => {\n  console.log("Context overflow!");\n});\n```\n\n### Capabilities detection\n\nIn all our above examples, we call `ai.languageModel.create()` and assume it will always succeed.\n\nHowever, sometimes a language model needs to be downloaded before the API can be used. In such cases, immediately calling `create()` will start the download, which might take a long time. The capabilities API gives you insight into the download status of the model:\n\n```js\nconst capabilities = await ai.languageModel.capabilities();\nconsole.log(capabilities.available);\n```\n\nThe `capabilities.available` property is a string that can take one of three values:\n\n* `"no"`, indicating the device or browser does not support prompting a language model at all.\n* `"after-download"`, indicating the device or browser supports prompting a language model, but it needs to be downloaded before it can be used.\n* `"readily"`, indicating the device or browser supports prompting a language model and it\u2019s ready to be used without any downloading steps.\n\nIn the `"after-download"` case, developers might want to have users confirm before you call `create()` to start the download, since doing so uses up significant bandwidth and users might not be willing to wait for a large download before using the site or feature.\n\nNote that regardless of the return value of `available`, `create()` might also fail, if either the download fails or the session creation fails.\n\nThe capabilities API also contains other information about the model:\n\n* `defaultTemperature`, `maxTemperature`, `defaultTopK`, and `maxTopK` properties giving information about the model\'s sampling parameters.\n* `languageAvailable(languageTag)`, which returns `"no"`, `"after-download"`, or `"readily"` to indicate whether the model supports conversing in a given human language.\n\n### Download progress\n\nIn cases where the model needs to be downloaded as part of creation, you can monitor the download progress (e.g. in order to show your users a progress bar) using code such as the following:\n\n```js\nconst session = await ai.languageModel.create({\n  monitor(m) {\n    m.addEventListener("downloadprogress", e => {\n      console.log(`Downloaded ${e.loaded} of ${e.total} bytes.`);\n    });\n  }\n});\n```\n\nIf the download fails, then `downloadprogress` events will stop being emitted, and the promise returned by `create()` will be rejected with a "`NetworkError`" `DOMException`.\n\n## Detailed design\n\n### Full API surface in Web IDL\n\n```webidl\n// Shared self.ai APIs\n\npartial interface WindowOrWorkerGlobalScope {\n  [Replaceable, SecureContext] readonly attribute AI ai;\n};\n\n[Exposed=(Window,Worker), SecureContext]\ninterface AI {\n  readonly attribute AILanguageModelFactory languageModel;\n};\n\n[Exposed=(Window,Worker), SecureContext]\ninterface AICreateMonitor : EventTarget {\n  attribute EventHandler ondownloadprogress;\n\n  // Might get more stuff in the future, e.g. for\n  // https://github.com/explainers-by-googlers/prompt-api/issues/4\n};\n\ncallback AICreateMonitorCallback = undefined (AICreateMonitor monitor);\n\nenum AICapabilityAvailability { "readily", "after-download", "no" };\n```\n\n```webidl\n// Language Model\n\n[Exposed=(Window,Worker), SecureContext]\ninterface AILanguageModelFactory {\n  Promise<AILanguageModel> create(optional AILanguageModelCreateOptions options = {});\n  Promise<AILanguageModelCapabilities> capabilities();\n};\n\n[Exposed=(Window,Worker), SecureContext]\ninterface AILanguageModel : EventTarget {\n  Promise<DOMString> prompt(AILanguageModelPromptInput input, optional AILanguageModelPromptOptions options = {});\n  ReadableStream promptStreaming(AILanguageModelPromptInput input, optional AILanguageModelPromptOptions options = {});\n\n  Promise<unsigned long long> countPromptTokens(AILanguageModelPromptInput input, optional AILanguageModelPromptOptions options = {});\n  readonly attribute unsigned long long maxTokens;\n  readonly attribute unsigned long long tokensSoFar;\n  readonly attribute unsigned long long tokensLeft;\n\n  readonly attribute unsigned long topK;\n  readonly attribute float temperature;\n\n  attribute EventHandler oncontextoverflow;\n\n  Promise<AILanguageModel> clone(optional AILanguageModelCloneOptions options = {});\n  undefined destroy();\n};\n\n[Exposed=(Window,Worker), SecureContext]\ninterface AILanguageModelCapabilities {\n  readonly attribute AICapabilityAvailability available;\n  AICapabilityAvailability languageAvailable(DOMString languageTag);\n\n  // Always null if available === "no"\n  readonly attribute unsigned long? defaultTopK;\n  readonly attribute unsigned long? maxTopK;\n  readonly attribute float? defaultTemperature;\n  readonly attribute float? maxTemperature;\n};\n\ndictionary AILanguageModelCreateOptions {\n  AbortSignal signal;\n  AICreateMonitorCallback monitor;\n\n  DOMString systemPrompt;\n  sequence<AILanguageModelInitialPrompt> initialPrompts;\n  [EnforceRange] unsigned long topK;\n  float temperature;\n};\n\ndictionary AILanguageModelInitialPrompt {\n  required AILanguageModelInitialPromptRole role;\n  required DOMString content;\n};\n\ndictionary AILanguageModelPrompt {\n  required AILanguageModelPromptRole role;\n  required DOMString content;\n};\n\ndictionary AILanguageModelPromptOptions {\n  AbortSignal signal;\n};\n\ndictionary AILanguageModelCloneOptions {\n  AbortSignal signal;\n};\n\ntypedef (DOMString or AILanguageModelPrompt or sequence<AILanguageModelPrompt>) AILanguageModelPromptInput;\n\nenum AILanguageModelInitialPromptRole { "system", "user", "assistant" };\nenum AILanguageModelPromptRole { "user", "assistant" };\n```\n\n### Instruction-tuned versus base models\n\nWe intend for this API to expose instruction-tuned models. Although we cannot mandate any particular level of quality or instruction-following capability, we think setting this base expectation can help ensure that what browsers ship is aligned with what web developers expect.\n\nTo illustrate the difference and how it impacts web developer expectations:\n\n* In a base model, a prompt like "Write a poem about trees." might get completed with "... Write about the animal you would like to be. Write about a conflict between a brother and a sister." (etc.) It is directly completing plausible next tokens in the text sequence.\n* Whereas, in an instruction-tuned model, the model will generally _follow_ instructions like "Write a poem about trees.", and respond with a poem about trees.\n\nTo ensure the API can be used by web developers across multiple implementations, all browsers should be sure their models behave like instruction-tuned models.\n\n## Alternatives considered and under consideration\n\n### How many stages to reach a response?\n\nTo actually get a response back from the model given a prompt, the following possible stages are involved:\n\n1. Download the model, if necessary.\n2. Establish a session, including configuring [per-session options](#configuration-of-per-session-options).\n3. Add an initial prompt to establish context. (This will not generate a response.)\n4. Execute a prompt and receive a response.\n\nWe\'ve chosen to manifest these 3-4 stages into the API as two methods, `ai.languageModel.create()` and `session.prompt()`/`session.promptStreaming()`, with some additional facilities for dealing with the fact that `ai.languageModel.create()` can include a download step. Some APIs simplify this into a single method, and some split it up into three (usually not four).\n\n### Stateless or session-based\n\nOur design here uses [sessions](#session-persistence-and-cloning). An alternate design, seen in some APIs, is to require the developer to feed in the entire conversation history to the model each time, keeping track of the results.\nThis can be slightly more flexible; for example, it allows manually correcting the model\'s responses before feeding them back into the context window.\nHowever, our understanding is that the session-based model can be more efficiently implemented, at least for browsers with on-device models. (Implementing it for a cloud-based model would likely be more work.) And, developers can always achieve a stateless model by using a new session for each interaction.\n\n'}}]);